## 모델 구하기 계획 (흐름에 맞춘 버전)

### 1. 파이프라인 단계를 모델 탐색 단위로 나누기

- **Step 1: 멀티모달 분류 모델**
    
    - 목적: 텍스트+이미지(or 영상/음성) → 카테고리 라벨/임베딩
        
    - 체크포인트: 입력 포맷, 분류 성능, 임베딩 추출 가능 여부
        
- **Step 2: DB 저장/검색 구조**
    
    - 목적: Vector DB, Category DB와 연결 가능한 출력 포맷
        
    - 체크포인트: 모델 출력 → 바로 벡터/라벨 저장 가능한가?
        
- **Step 3: 트렌드 추출 모델**
    
    - 목적: DB 내 벡터/라벨에서 시계열·빈도 기반 패턴 감지
        
    - 체크포인트: 통계적 방법 vs LLM 기반 클러스터링 중 선택
        
- **Step 4: 스토리 생성 모델**
    
    - 목적: 트렌드 키워드 입력 → 내러티브 생성
        
    - 체크포인트: 프롬프트 제어 가능 여부, 길이/스타일 다양성
        
- **Step 5: 멀티모달 생성 모델 (영상/음성)**
    
    - 목적: 생성된 스토리 → 영상/음성/배경음악
        
    - 체크포인트: 출력 파일 형식, 리소스 요구사항
        

---

### 2. 각 단계별 모델 탐색 계획

- HuggingFace → Task별 키워드 검색
    
- Papers with Code → 성능 비교 (특히 멀티모달/스토리 분야)
    
- GitHub → 오픈소스 실행 가능성 확인
    

👉 우선은 **각 단계별로 후보 2~3개만** 모읍니다. (너무 많으면 진행이 안 됨)

---

### 3. 적합성 평가 플로우

모델을 찾을 때마다 **동일한 기준표**로 평가:

1. Input/Output 포맷이 내 파이프라인에 맞는가?
    
2. 내 환경(GPU/Colab/서버)에서 돌릴 수 있는가?
    
3. 오픈소스 라이선스 제약은 없는가?
    
4. 커뮤니티/문서화가 충분해서 유지보수가 가능한가?
    

---

### 4. 점진적 검증 전략

- **Phase 1**: Step1+Step2 (멀티모달 분류 → DB 저장) 먼저 구현 → 데이터가 쌓여야 다음 단계 가능
    
- **Phase 2**: Step3 (트렌드 추출) 간단한 빈도 분석으로 시작
    
- **Phase 3**: Step4+Step5 (스토리+멀티모달 생성) 붙이면서 전체 플로우 완성

# ----------------------------------------------------------

## 6주 일정 (평일/주말 포함)

### 1주차 — “찾고 모으기”

- **평일(25h)**: 키워드 검색 → 후보 12~16개 수집, 비교표 라인 생성, 첫인상 기록
    
- **토(10h)**: 후보 정리(비교표 입력), 용어 학습(CLIP/BLIP/Diffusion/TTS)
    
- **일(10h)**: 설치 스모크 테스트(멀티모달 1, 스토리 1) + 실패 로그 기록

## 1주차 후보 설정 방식 (입문자 기준)

### 1단계. 키워드 검색

- HuggingFace / Papers with Code / GitHub에서 각각 같은 키워드로 검색
    
- 멀티모달, 스토리, 음성, 영상 각 분야별로 **상위 1~2페이지** 정도만 확인
    

---

### 2단계. 1차 필터링 (검색 직후 바로 확인)

후보를 고를 때 **아주 간단한 기준**으로 거릅니다:

1. **최근 업데이트**: 1년 이상 방치된 레포는 제외
    
2. **설치/사용법이 명확**: README에 Quickstart 코드가 있는지
    
3. **입출력이 내 Task와 유사**: 예) “image+text”인데 영상만 지원하면 패스
    
4. **License 확인**: MIT/Apache-2.0/CC-BY 계열 위주
    

👉 이 기준으로 하면 검색 결과의 30~40%만 남습니다.x

---

### 3단계. 후보 수집

- 각 Task(멀티모달/스토리/TTS/영상)마다 **3~4개씩** 뽑기
    
- 즉, 총 **12~16개 모델** 정도가 1주차 “후보군”이 됩니다.
    
- 이걸 비교표 CSV에 간단히 입력 (모델 이름, 링크, 첫인상 메모)
    

---

### 4단계. 주말 작업

- 평일에 모은 후보를 **비교표에 정식 입력** (Input/Output, 설치 요구사항 등)
    
- 1~2개 정도는 설치 스모크 테스트까지 시도
    

---

### 2주차 — “문서 분석 + 설치 1차”

- **평일(25h)**: 후보 좁히기(분야별 3→2개), README 분석, 미니 데이터셋 준비
    
- **토(10h)**: 설치 1차(멀티모달 2, 스토리 2), 샘플 입출력 확인
    
- **일(10h)**: 설치 결과 정리 + 비교표 업데이트, 추가 설치 리트라이
    

---

### 3주차 — “평가 기준 확정 + 설치 2차”

- **평일(25h)**: 평가 기준 문서화, 간단 파이썬 평가 스크립트 작성
    
- **토(10h)**: 설치 2차(TTS 2, 영상 2)
    
- **일(10h)**: 전 모델(8개) 추론 스모크 완료, 공통 평가 스크립트 연결
    

---

### 4주차 — “벤치(1): 멀티모달·스토리”

- **평일(25h)**: 멀티모달 2개 벤치(F1, 속도), 스토리 2개 벤치(룹릭 평가)
    
- **토(10h)**: 결과 정리, 우선순위 1위/2위 선정
    
- **일(10h)**: 1위 모델 출력 포맷 → JSON/Vec 어댑터 작성
    

---

### 5주차 — “벤치(2): TTS·영상”

- **평일(25h)**: TTS 2개 벤치(MOS 평가), 영상 2개 벤치(품질 룹릭, 생성시간)
    
- **토(10h)**: 결과 정리, TTS·영상 1위 선정
    
- **일(10h)**: 출력 포맷 정규화 스크립트(mp3/mp4+메타 JSON)
    

---

### 6주차 — “최소 통합(MVP)”

- **평일(25h)**: 엔드투엔드 연결(분류→DB→트렌드→스토리→TTS/영상)
    
- **토(10h)**: 샘플 10~20건 리허설, 실패 수정
    
- **일(10h)**: 최종 1위 모델 4개 확정, README 문서화(설치–실행–평가–통합 재현)
