# NeXt-TDNN: 화자 검증을 위한 다중 스케일 시간 지연 신경망 백본의 현대화

저자: 허현준, 신의협, 이란, 천영주, 박형민
서강대학교 전자공학과, 서울, 대한민국
현대자동차, 서울, 대한민국
(공종 제1저자, 교신서자)

## 초록

화자 검증 분야에서 ECAPA-TDNN은 **1차원(1D) Res2Net 블록**, **squeeze-and-excitation(SE) 모듈**, 그리고 **다중 계층 특징 집계(MFA)** 를 활용하여 놀라운 성능 향상을 보여주었습니다. 
한편, 비전(vision) 분야에서는 ConvNet 구조가 Transformer를 참조하여 현대화되면서 성능이 크게 향상되었습니다
본 논문에서는 화자 검증을 위한 TDNN의 블록 설계를 개선하여 제안합니다. 최신 ConvNet 구조에서 영감을 받아, ECAPA-TDNN의 **SE-Res2Net 블록**을 새롭게 설계한 **1차원 2단계 다중 스케일 ConvNeXt 블록(TS-ConvNeXt)** 으로 교체했습니다.

TS-ConvNeXt 블록은 두 개의 독립된 하위 모듈로 구성됩니다:

- **시간적 다중 스케일 합성곱(MSC, Multi-Scale Convolution)**
    
- **프레임 단위 피드포워드 네트워크(FFN)**

이러한 2단계 설계는 **프레임 간(inter-frame)과 프레임 내(intra-frame) 문맥**을 유연하게 포착할 수 있도록 합니다. 또한, ECAPA-TDNN의 SE 모듈과 유사하게, FFN 모듈에 **Global Response Normalization(GRN)** 을 도입하여 선택적인 특징 전달을 가능하게 했습니다

실험 결과, 현대화된 백본 블록을 사용하는 NeXt-TDNN은 화자 검증 작업에서 **성능을 크게 향상**시키면서도 **파라미터 크기와 추론 시간은 줄이는 효과**를 보였습니다.  

# ------------------------------------------------------------
## 1. 서론

심층 신경망(Deep Neural Networks)의 부상과 함께, 기존의 사람이 설계한 화자 식별용 임베딩 특징 벡터인 **i-vector**\[[1](#^ref1)\]는 빠르게 DNN 기반 벡터인 **d-vector**\[[2](#^ref2)\]로 대체되었습니다. 
특히, **x-vector**\[[3](#^ref2)]는 **시간 지연 신경망(TDNN)** 을 활용하여 성능을 크게 향상시켰으며, 그 이후 다양한 개선 방법들이 연구되었습니다\[[4](#^ref4), [5](#^ref5)].

최근에는 **ECAPA-TDNN**\[[6](#^ref6)\]이 제안되었는데, 이는 TDNN 아키텍처를 개선하여 최신 성능(state-of-the-art)을 달성했습니다.

- 다중 스케일의 스펙트럼 구조를 포착하기 위해, 1차원(1D) 합성곱 기반 **Res2Net**\[[7](#^ref7)\]을 
  백본 레이어로 사용했습니다.
    
- 또한, **SE(squeeze-and-excitation) 블록**\[[8](#^ref8)\]을 사용하여 전체 시간적 문맥에서의 
  특징 게이팅을 수행했습니다.
    
- 마지막으로, 시간 풀링 이전에 얕은 층의 특징까지 활용할 수 있도록 
  **MFA(Multi-layer Feature Aggregation)** 구조를 도입했습니다.
 
따라서 ECAPA-TDNN은 화자 검증 작업의 표준 모델이 되었으며, 주요 연구들의 베이스라인으로 활용되고 있습니다\[[9](#^ref9), [10](#^ref10), [11](#^ref11)\].

그러나, 음성은 **스펙트럼 기반의 2차원(2D) 시각적 특징**으로도 볼 수 있습니다. 
따라서 화자 임베딩 벡터를 추출하기 위해 간단한 **2D ResNet 기반 아키텍처**\[[12](#^ref12)\]가 종종 사용되며, 화자 검증에서도 안정적인 결과를 보였습니다\[[13](#^ref13), [14](#^ref14), [15](#^ref15), [16](#^ref16)].

한편, 전통적인 비전(vision) 작업에서는 ResNet을 포함한 ConvNet 구조가 **비전 Transformer**\[[17](#^ref17)\]의 등장 이후 지속적으로 개선되고 현대화되었습니다.
**ConvNeXt**\[[18](#^ref18)\]는 이러한 개선 사례 중 하나로, Transformer의 아키텍처를 참고한 구조입니다. 이후 연구에서는 **ConvNeXt-V2**\[[19](#^ref19)\]가 제안되었고, 여기서 **Global Response Normalization(GRN)** 이 도입되어 전역 공간 문맥에서 채널 차원을 강조했습니다.

본 논문에서는 최신 ConvNeXt 구조에서 영감을 받아, TDNN을 위한 현대화된 **백본 블록을 제안**합니다. 

Fig. 1
![[Pasted image 20251016100200.png]]^img1

\[[Fig. 1](#^img1)\]에서 볼 수 있듯이, 
ConvNeXt 블록을 다시 설계하여 **SE-Res2Net 블록을 대체할 수 있는 
2단계 ConvNeXt(TS-ConvNeXt) 블록**을 만들었습니다.

TS-ConvNeXt 블록은 두 개의 하위 모듈로 구성됩니다:

1. **다중 스케일 합성곱(MSC, Multi-Scale Convolution) 모듈** 
   → 서로 다른 스케일을 가진 병렬 1D depth-wise convolution(DConv1D) 레이어 기반.
    
2. **피드포워드 네트워크(FFN) 모듈** → Transformer 구조처럼 프레임 단위로 배치.
    

추가적으로, FFN 내부에 **GRN(Global Response Normalization)**\[[19](#^ref19)\]을 적용하여 
채널 대비(contrast)를 강화했으며, 이는 ECAPA-TDNN의 SE 모듈을 대체할 수 있습니다.

제안된 TS-ConvNeXt 블록을 기반으로, 저희는 더 개선된 화자 임베딩 벡터를 추출할 수 있는 
현대화된 **NeXt-TDNN**을 개발했습니다

---
## 2. 제안된 NeXt-TDNN 아키텍처
### 2.1 MFA 레이어와 ASP 기반 시간적 풀링

[Fig. 1](#^img1)에서 보듯이, inpur feature은 크기 `Cₘₑₗ × T` 의 **Mel-스펙트로그램**으로 주어지며, 
여기서 T는 프레임의 개수입니다. 

이 입력은 커널 크기 4의 표준 1D 합성곱 층을 통과하면서 스펙트럼 차원 (Cₘₑₗ)을 
잠재 채널 차원 (C)으로 변환합니다.

이후 출력 특징 F⁽⁰⁾ ∈ ℝᴰ (= ℝᶜ×ᵀ) 은 3단계의 TS-ConvNeXt 블록을 통과합니다. 
각 단계의 출력 F⁽¹⁾, F⁽²⁾, F⁽³⁾ ∈ ℝᶜ×ᵀ 을 모두 통합하기 위해, ECAPA-TDNN\[[6](#^ref6)\]과 유사하게 
**MFA (Multi-layer Feature Aggregation)** 레이어를 사용했습니다.

구체적으로, 이 세 출력은 단순 연결(concatenation)되어 F ∈ ℝ³ᶜ×ᵀ 이 되고, 
그 후 **1D Point-wise Convolution (PConv1D)** 으로 처리되어 출력 차원 Cₘₑₗₐ 를 얻습니다. 
이후 **Layer Normalization (LN)** 이 적용됩니다.

발화 단위(utterance-level)의 임베딩을 프레임 단위 특징으로부터 추출하기 위해, 
ECAPA-TDNN 과 동일하게 **ASP (Attentive Statistics Pooling)**\[[20](#^ref20)\] 층을 사용했습니다.  
ASP 층은 프레임별 특징 hₜ로부터 0과 1 사이의 **채널 의존 가중치 값 αₜ ∈ ℝᶜₘfₐ×¹** 을 계산합니다.

이 가중치를 활용해 프레임별 가중 평균과 표준편차를 다음과 같이 구합니다:

$$\mu = \sum_{t=1}^{T} \alpha_t \odot h_t,\quad \sigma = \sqrt{\sum_{t=1}^{T}\alpha_t \odot h_t \odot h_t - \mu \odot \mu}$$

여기서 ⊙ 는 Hadamard (요소별) 곱을 의미합니다.  
이렇게 얻은 평균 (μ)과 표준편차 (σ) 벡터를 연결한 후 **선형 층(Linear Layer)** 을 통해 
최종 화자 임베딩 x ∈ ℝ¹⁹²×¹ 을 추출합니다.

```
NeXt-TDNN 전체 구조 요약:
Mel-스펙트로그램 입력에서 TS-ConvNeXt 3단 스테이지를 거쳐 MFA → ASP → Linear 출력
```


### 2.2 제안된 TS-ConvNeXt 블록

[Fig. 1](#^img1)에서 보이듯, 백본은 3단계의 TS-ConvNeXt 블록으로 구성되며, 각 단계마다 블록이 B번 반복됩니다.  
n번째 스테이지(n ∈ {1, 2, 3})에서 b번째 블록(1 ≤ b ≤ B)의 입력 표현 $F⁽ⁿ⁻¹⁾_{b-1} ∈ ℝᶜ×ᵀ$ 은 다음과 같이 처리됩니다:

$$\begin{align} \tilde{F}_{b-1}^{(n-1)} = F_{b-1}^{(n-1)} + MSC(F_{b-1}^{(n-1)}) \tag{1}\\ F_{b}^{(n-1)} = \tilde{F}_{b-1}^{(n-1)} + FFN(\tilde{F}_{b-1}^{(n-1)}) \tag{2} \end{align}$$
여기서 F⁽⁰⁾₀ = F⁽⁰⁾ 입니다.  
B번째 블록의 출력 F⁽ⁿ⁻¹⁾_B 는 **MFA 레이어의 입력**이자 **다음 스테이지의 첫 입력 F⁽ⁿ⁾₀** 가 됩니다.

MSC 모듈은 **프레임 간(inter-frame) 시간적 문맥**을 포착하는 반면, FFN 모듈은 **프레임별 특징(frame-wise)** 을 독립적으로 처리합니다.

Fig. 2
![[Pasted image 20251016130654.png]]^img2

다중 스케일 특징을 효과적으로 포착하기 위해, 
[Fig. 2](#^img2)(a)의 기존 Res2Net\[[7](#^ref7)\] 대신 MSC 모듈 내에 **s개의 병렬 DConv1D 레이어**를 사용했습니다.  
각 레이어는 서로 다른 커널 크기 K₁, …, Kₛ 를 가지며, 스케일 팩터 s 에 따라 다중 스케일 정보를 포착합니다 [Fig. 2](#^img2)(b).

각 스케일 별 훈련을 안정화하기 위해, 
MSC 모듈 inpur feature은 각 DConv1D 앞에서 차원을 C′ 로 축소(projection) 합니다.  
이후 생성된 s개의 다중 스케일 출력들을 연결(concat) 하여 차원을 s·C′ 로 만든 뒤, 
**GELU 활성화** 후 **PConv1D** 로 다시 C차원으로 투사합니다.

MSC의 작동 원리는 단순하지만, Transformer의 **multi-head self-attention(MHSA)** 과 유사합니다.  
단, 차이는 MHSA의 attention 연산을 MSC에서는 **DConv1D** 로 대체했다는 점입니다.

한편, FFN 은 확장 계수 4를 가지는 두 개의 PConv1D 층과 GELU 활성화를 포함합니다.  
또한, 채널별 대비(contrast)를 강화하기 위해 FFN 내에 **GRN**\[[19](#^ref19)\]을 도입했습니다.  
GRN은 추가적인 파라미터 층이 필요 없는 간단하면서 효율적인 정규화 기법입니다.

구체적으로, GELU 활성화 이후 은닉 특징 G = \[g₁, …, g_T\] ∈ ℝ⁴ᶜ×ᵀ 이라 하면,  
GRN은 시간 축에 대한 L2-norm을 계산해 벡터 ḡ ∈ ℝ⁴ᶜ×¹ 을 만들고,  
이를 L1-norm으로 정규화한 응답 정규화 함수 N(·) 을 적용합니다:
$$N(\bar{g} = \frac{\bar{g}}{\Vert{\bar{g}}\Vert_1})$$
그 다음, 정규화된 값을 활용하여 **skip connection** 형태로 채널 보정이 이뤄집니다:

$$GRN(g_t) = g_t + \gamma\; \odot N(\bar{g}) \odot g_t + \beta \tag{3}$$

여기서 γ, β ∈ ℝ⁴ᶜ×¹ 은 학습 가능한 Affine 변환 파라미터이며, 초기값은 0으로 설정됩니다.  
이로써 GRN은 학습 초기에는 단순 bypass 처럼 동작하다가, 훈련이 진행되면서 점진적으로 적응합니다.

(주 2) 실제 구현에서는 정규화 값이 채널 차원 4C 로 스케일링됩니다\[[19](#^ref19)\].

```
MSC 요약:
서로 다른 커널 크기를 갖는 병렬 DConv1D 를 이용하여 다중 시간 스케일 정보를 동시에 포착
```



### 2.3 백본 블록 설계의 근거

Fig. 3![[Pasted image 20251016131758.png]]^img3

[Fig. 3](#^img3)은 기존과 제안된 여러 블록 구조를 비교하여 보여줍니다.

먼저 **[Fig. 3](#^img3)(a)** 의 **SE-Res2Net 블록**\[[7](\^ref7)\] 은 기존 ResNet 구조를 따르며,  
채널 차원을 조절하기 위한 두 개의 **PConv2D 레이어**로 구성되어 있습니다.  
각 레이어 뒤에는 **Batch Normalization(BN)** 과 **ReLU 활성화 함수**가 이어집니다.  
기존 ResNet 의 표준 합성곱은 **Res2 Dilated Conv1D**([Fig. 2](#^img2)(a) 참조)로 대체되어 
다중 스케일 문맥을 포착하고,  특정 채널에 주의를 집중시키기 위해 **SE 블록**을 추가했습니다.

다음으로 **Transformer**\[[21](#^ref21)\]는 **MHSA(Multi-Head Self-Attention)** 과 **FFN(Feed-Forward Network)** 의 두 개 서브모듈로 구성됩니다([Fig. 3](#^img3)(b)).

이에 영감을 받아, **ConvNeXt(-V2)** 블록\[[18](#^ref18), [19](#^ref19)\]([Fig. 3](#^img3)(c)) 은 ResNet 블록을 Transformer 구조에 맞춰 변경한 형태로 설계되었습니다.  
특히 ResNet 의 표준 합성곱을 **DConv(Depth-wise Conv)** 로 바꾸고, 큰 커널 크기를 사용하여 Transformer 의 MHSA 역할을 유사하게 수행하도록 했습니다.  
또한 Transformer 의 FFN 처럼 정규화와 활성화를 줄인 **inverted bottleneck 구조**를 적용했습니다.

ConvNeXt-V2\[[19](#^ref19)\] 에서는 새롭게 **GRN(Global Response Normalization)** 을 제안하여 
GELU 활성화 뒤의 inverted bottleneck 구간에 적용함으로써 성능을 향상시켰습니다.

이러한 관점을 바탕으로 ECAPA-TDNN 의 SE-Res2Net 을 현대화하기 위해,  
우리는 **ConvNeXt** 를 기초로 **TS-ConvNeXt** 를 설계했습니다([Fig. 3](#^img3)(d)).  
구체적으로, **MSC(다중 스케일 합성곱)** 의 시간적 처리 블록과 **FFN** 의 위치별 처리 블록을 분리하여 **2단계(two-step)** 로 작동하도록 했고,  
MSC 모듈 내에는 시간적 다중 스케일 특징을 도입했습니다.  
이 변형된 디자인은 Transformer 구조와 더 유사해지며,  
**프레임 간(inter-frame)** 및 **프레임 내(intra-frame)** 문맥 학습의 유연성을 높입니다.

SE-Res2Net 관점에서 보면, MSC 는 다중 스케일 시간 모델링을 위해 Res2Net 을 대체하고,  
**FFN + GRN** 조합은 SE 블록을 대체하여 위치별 특징 처리를 담당할 수 있습니다.  
추가로, **TS-ConvNeXt-l**([Fig. 3](#^img3)(e)) 은 경량화 버전으로 설계되었으며, MSC 모듈 대신 단일 **DConv1D 레이어**만을 사용합니다.

---
## 3. 실험 설정 (EXPERIMENTAL SETUP)

### 3.1 데이터셋과 평가 지표

본 연구의 학습 및 평가에는 **VoxCeleb1**\[[22](#^ref22)\] 과 **VoxCeleb2**\[[13](#^ref13)\] 데이터셋을 사용했습니다.

- **VoxCeleb1** 은
    
    - 개발 세트 (development set): 1,211명 화자
        
    - 평가 세트 (evaluation set): 40명 화자
    
- **VoxCeleb2** 는
    
    - 개발 세트: 5,994명 화자
        
    - 평가 세트: 118명 화자
    

**학습 데이터셋**으로는 VoxCeleb2의 개발 세트를 사용했고,  
**평가 데이터셋**으로는 VoxCeleb1에서 다음 세 가지 subset을 사용했습니다:

1. **VoxCeleb1-O**: 원래의 테스트 세트만 포함
    
2. **VoxCeleb1-E**: 개발 + 테스트 세트 전체 포함
    
3. **VoxCeleb1-H**: VoxCeleb1-E 중 동일한 **국적 및 성별** 화자만 선택한 subset
    

모델의 성능 평가는 다음 두 지표로 이루어졌습니다:

- **EER (Equal Error Rate)** — 오탐률과 미탐률이 동일한 지점의 오류율
    
- **minDCF (Minimum Detection Cost Function)** — P_target = 0.01, C_FA = C_Miss = 1 조건에서의 최소 검출 비용 함수
    

임베딩 쌍 간의 **코사인 거리**(cosine distance) 로 점수를 계산했으며,  
점수 정규화에는 **adaptive s-norm**\[[23](#^ref23)\]기법을 적용했습니다.

또한 훈련 세트에서 6,000개의 발화를 **임포스터(imposter) 코호트**로 사용하고,  
상위 300개의 임포스터 점수를 score normalization에 활용했습니다.


### 3.2 설정 (Configurations)

input 특징으로는, **80차원 로그-Mel 필터뱅크(log-Mel filterbanks)** 를 사용했습니다 
($C_{mel} = 80$). 

이 feature는

- **FFT 크기 512**,
    
- **25ms Hamming 윈도우**,
    
- **10ms 프레임 시프트**  
    조건에서 스펙트로그램으로부터 추출했습니다.
    

훈련 시, 3초 길이의 구간을 임의로 잘라(random crop) 사용했으며,  
손실 함수로는 **AAM-softmax (Additive Angular Margin Softmax)**\[[24](#^ref24)\]를 사용했습니다.  

- margin = 0.3
	
- scale = 40

제안된 NeXt-TDNN에서는 MFA 레이어의 출력 차원 C_MFA 를 입력과 동일한 3C 로 설정했고,  
TS-ConvNeXt 블록의 MSC 모듈에서 프로젝션 차원은 C′ = C/s 로 설정했습니다.

또한 ECAPA-TDNN\[[6](#^ref6)\]의 설정을 따르며, **Kaldi recipe**\[[4](#^ref4)\]를 사용해 데이터 증강을 수행했습니다.

- **MUSAN**\[[25](#^ref25)\]: 잡음, 음악, 음성 데이터로 노이즈 증강
    
- **RIR**\[[26](#^ref26)\]: 실내 잔향(Room Impulse Response) 증강
    
- **SpecAugment**\[[27](#^ref27)\]: 스펙트로그램 마스킹 기법 적용
    

배치 크기는

- Mobile 모델의 경우 **500**,
    
- Base 모델의 경우 **300** 으로 설정했습니다.
    

학습률은 각각 **5×10⁻⁴** (모바일 모델) 및 **3×10⁻⁴** (베이스 모델)에서 시작하여,  
**10 epoch마다 0.8배 감소**시켰습니다\[[28](#^ref28)\].

최대 200 epoch까지 학습했으며, 옵티마이저는 **AdamW** 를 사용했습니다.  
가중치 감쇠(weight decay)는 **0.01**,  
안정적인 학습을 위해 **L2-norm 1 이하의 gradient clipping**을 적용했습니다.

---

Table. 1
![[Pasted image 20251016135215.png]]^img4
_VoxCeleb-O 기반으로 C = 256, B = 3 설정된 백본 구조에 따른 NeXt-TDNN 모델에 대한 평가.
$K$는 DConv1D 레이어의 커널 크기(또는 커널 크기 집합)를 의미합니다.

Table/ 2 ![[Pasted image 20251016165354.png]]^img5
_VoxCeleb1-O, E, H 데이터셋 기반으로 기존 모델과 제안된 모델의 성능 평가.  

- _기존 모델로는 Fast ResNet-34\[[16](#^ref16)\], ECAPA-TDNN\[[6](#^ref6)\]¹, 그리고 ECAPA-TDNN의 개선 버전인 EfficientTDNN\[[9](#^ref9)\]²를 사용하였다.
- _NeXt-TDNN-l 모델의 TS-ConvNeXt-l 블록에서 커널 크기 $K$는 65로 설정되었다.  
- _NeXt-TDNN의 TS-ConvNeXt 내 MSC 모듈에서는 스케일 팩터를 $s = 2$,
  다중 스케일 커널 크기 집합을 $[7,65]$로 각각 설정하였다.  

_MACs와 RTF는 3초 길이의 세그먼트에서 측정되었으며,  
RTF는 NVIDIA GeForce GTX 1080 Ti GPU에서 충분히 반복된 추론 환경을 통해 측정되었다.

## 4. 실험 결과 (EXPERIMENTAL RESULT)

먼저 **TS-ConvNeXt 블록의 유효성**을 검증한 결과는 **[Table 1](#^img4)** 에 요약되어 있습니다.

- 일반적으로 **GRN(Global Response Normalization)** 을 적용하면, ECAPA-TDNN의 SE 블록처럼 **전역 시간 문맥 기반으로 프레임 단위 특징을 강조**하여 EER 측면의 성능이 향상되었습니다.
    
- 또한 ConvNeXt와 달리, **TS-ConvNeXt-l** 은 커널 크기 **K=65** 일 때 성능이 특히 개선되었으며,  
    이는 ConvNeXt를 단순히 두 개의 서브모듈(MSC, FFN)로 분리한 설계가 **큰 커널을 사용할 때 효과적임**을 보여줍니다.
    
- 나아가, **TS-ConvNeXt** 블록에서 작은 커널과 큰 커널을 **다중 스케일(multi-scale)** 로 결합했을 때, 단일 커널(65)보다 더욱 향상된 결과를 얻었습니다.



(¹우리는 **SpeechBrain**\[[29](#^ref29)\] 에서 구현된 소스 코드를 사용하여 **ECAPA-TDNN** 모델을 학습하고 평가했습니다.)
(²우리는 **EfficientTDNN** 모델의 경우, 저자가 제공한 **공식 사전 학습 모델**을 사용하여 평가했습니다.)

---
## **5. 결론 (CONCLUSION)**

본 연구에서는 화자 검증을 위한 TDNN 구조를 **ECAPA-TDNN** 기반에서 **현대화(modernized)** 하는 것을 목표로 했습니다.

Transformer 및 최신 ConvNet 아키텍처의 구조에서 영감을 받아,  
ECAPA-TDNN의 핵심 블록인 **SE-Res2Net** 을 새롭게 설계한 **TS-ConvNeXt** 블록으로 대체했습니다.

이 블록은 두 개의 독립된 하위 모듈로 구성됩니다:

- **MSC (Multi-Scale Convolution)** — 시간적 다중 스케일 정보를 포착
    
- **FFN (Feed-Forward Network)** — 프레임 단위 채널 문맥(position-wise context)을 처리
    

또한, FFN 모듈 내부에 **GRN (Global Response Normalization)** 을 도입하여 채널 간 대비(feature contrast)를 강화했습니다.

실험 결과, 현대화된 TS-ConvNeXt 블록을 채용한 **NeXt-TDNN** 은  
화자 검증(speaker verification) 과제에서 **효율적이면서도 높은 성능을 달성**함을 확인했습니다.

---
## 6. References

[1] Najim Dehak, Patrick J. Kenny, Reda Dehak, Pierre Du- ´mouchel, and Pierre Ouellet, “Front-end factor analysis for speaker verification,” IEEE TASLP, vol. 19, no. 4, pp. 788–798, 2011. ^ref1

[2] Ehsan Variani, Xin Lei, Erik McDermott, Ignacio Lopez Moreno, and Javier Gonzalez-Dominguez, “Deep neural networks for small footprint text-dependent speaker verification,” in Proc. of ICASSP, 2014, pp. 4052–4056. ^ref2

[3] David Snyder, Daniel Garcia-Romero, Gregory Sell, Daniel Povey, and Sanjeev Khudanpur, “X-vectors: Robust DNN embeddings for speaker recognition,” in Proc. of ICASSP, 2018, pp. 5329–5333. ^ref3

[4] David Snyder, Daniel Garcia-Romero, Gregory Sell, Alan McCree, Daniel Povey, and Sanjeev Khudanpur, “Speaker recognition for multi-speaker conversations using x-vectors,” in Proc. of ICASSP, 2019, pp. 5796–5800. ^ref4

[5] Daniel Garcia-Romero, Alan McCree, David Snyder, and Gregory Sell, “Jhu-HLTCOE system for the voxsrc speaker recognition challenge,” in Proc. of ICASSP, 2020, pp. 7559–7563. ^ref5

[6] Brecht Desplanques, Jenthe Thienpondt, and Kris Demuynck, “ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification,” in
Proc. Interspeech, 2020, pp. 3830–3834. ^ref6

[7] Shang-Hua Gao, Ming-Ming Cheng, Kai Zhao, Xin-Yu Zhang, Ming-Hsuan Yang, and Philip Torr, “Res2Net: A new multiscale backbone architecture,” IEEE TPAMI, vol. 43, no. 2, pp. 652–662, 2021. ^ref7

[8] Jie Hu, Li Shen, and Gang Sun, “Squeeze-and-excitation networks,” in Proc. of CVPR, June 2018. ^ref8

[9] Rui Wang, Zhihua Wei, Haoran Duan, Shouling Ji, Yang Long, and Zhen Hong, "EfficientTDNN: Efficient architecture search for speaker recognition,” IEEE/ACM TASLP, vol. 30, pp. 2267–2279, 2022. ^ref9

[10] Jee weon Jung, Youjin Kim, Hee-Soo Heo, Bong-Jin Lee, Youngki Kwon, and Joon Son Chung, “Pushing the limits of raw waveform speaker recognition,” in Proc. Interspeech,
2022, pp. 2228–2232. ^ref10

[11] Yang Zhang, Zhiqiang Lv, Haibin Wu, Shanshan Zhang, Pengfei Hu, Zhiyong Wu, Hung yi Lee, and Helen Meng, “MFA-Conformer: Multi-scale Feature Aggregation Conformer for Automatic Speaker Verification,” in Proc. Interspeech, 2022, pp. 306–310. ^ref11

[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, “Deep residual learning for image recognition,” in Proc. of CVPR, June 2016. ^ref12

[13] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman, “VoxCeleb2: Deep speaker recognition,” in Proc. Interspeech, 2018, pp. 1086–1090. ^ref13

[14] Weicheng Cai, Jinkun Chen, and Ming Li, “Exploring the encoding layer and loss function in end-to-end speaker and language recognition system,” in Proc. Odyssey 2018 The Speaker
and Language Recognition Workshop, 2018, pp. 74–81. ^ref14

[15] Hossein Zeinali, Shuai Wang, Anna Silnova, Pavel Matejka, ˇ and Oldˇrich Plchot, “BUT system description to voxCeleb speaker recognition challenge 2019,” 2019. ^ref15

[16] Joon Son Chung, Jaesung Huh, Seongkyu Mun, Minjae Lee, Hee-Soo Heo, Soyeon Choe, Chiheon Ham, Sunghwan Jung, Bong-Jin Lee, and Icksang Han, “In Defence of Metric Learning
for Speaker Recognition,” in Proc. Interspeech, 2020, pp. 2977–2981. ^ref16

[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
Jakob Uszkoreit, and Neil Houlsby, “An image is worth 16x16 words: Transformers for image recognition at scale,” in International Conference on Learning Representations, 2021. ^ref17

[18] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie, “A Convnet for the 2020s,” in Proc. of CVPR, June 2022, pp. 11976–11986. ^ref18

[19] Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, and Saining Xie, “ConvNext V2: Co-designing and scaling ConvNets with masked autoencoders,” in Proc. of CVPR, June 2023, pp. 16133–16142. ^ref19

[20] Koji Okabe, Takafumi Koshinaka, and Koichi Shinoda, “Attentive statistics pooling for deep speaker embedding,” in Proc. Interspeech, 2018, pp. 2252–2256. ^ref20

[21] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin, “Attention is all you need,” in Advances in Neural Information Processing Systems, I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds. 2017, vol. 30, Curran Associates, Inc. ^ref21

[22] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman, “VoxCeleb: A Large-Scale Speaker Identification Dataset,” in Proc. Interspeech, 2017, pp. 2616–2620. ^ref22

[23] Pavel Matejka, Ond ˇ ˇrej Novotny, Old ´ ˇrich Plchot, Luka´s Burget, ˇ Mireia Diez Sanchez, and Jan ´ Cernock ˇ y, “Analysis of score ´normalization in multilingual speaker recognition,” in Proc. Interspeech, 2017, pp. 1567–1571. ^ref23

[24] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou, “Arcface: Additive angular margin loss for deep face recognition,” in Proc. of CVPR, June 2019. ^ref24

[25] David Snyder, Guoguo Chen, and Daniel Povey, “Musan: A music, speech, and noise corpus,” 2015. ^ref25

[26] J. B. Alien and D. A. Berkley, “Image method for efficiently simulating small-room acoustics,” The Journal of the Acoustical Society of America, vol. 60, no. S1, pp. S9–S9, 08 2005. ^ref26

[27] Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D. Cubuk, and Quoc V. Le, “SpecAugment: A Simple Data Augmentation Method for Automatic Speech
Recognition,” in Proc. Interspeech, 2019, pp. 2613–2617. ^ref27

[28] Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, ´ Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He, “Accurate, large minibatch sgd: Training
imagenet in 1 hour,” 2018. ^ref28

[29] Mirco Ravanelli, Titouan Parcollet, Peter Plantinga, Aku Rouhe, Samuele Cornell, Loren Lugosch, Cem Subakan, Nauman Dawalatabad, Abdelwahab Heba, Jianyuan Zhong,
Ju-Chieh Chou, Sung-Lin Yeh, Szu-Wei Fu, Chien-Feng Liao, Elena Rastorgueva, Franc¸ois Grondin, William Aris, Hwidong Na, Yan Gao, Renato De Mori, and Yoshua Bengio,
“SpeechBrain: A general-purpose speech toolkit,” 2021, arXiv:2106.04624. ^ref29