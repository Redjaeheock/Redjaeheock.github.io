
# Requirement

| íŒ¨í‚¤ì§€                         | ì—­í•  / ì„¤ëª…                                                  |
| --------------------------- | -------------------------------------------------------- |
| **flash-attn==2.7.4.post1** | FlashAttention ì»¤ë„ ê°€ì†. A100/H100ìš©. <br>(MX450 ë“±ì—ì„œëŠ” ì‚¬ìš© ë¶ˆê°€) |
| **torch==2.6.0**            | PyTorch í•µì‹¬ ì—°ì‚° ë¼ì´ë¸ŒëŸ¬ë¦¬                                      |
| **transformers==4.48.2**    | Phi-4 ê³„ì—´ì´ í†µí•©ëœ ìµœì‹  ë²„ì „ (4.48.0 ì´ìƒ í•„ìˆ˜)                       |
| **accelerate==1.3.0**       | CPU/GPU ìë™ ë¶„ë°° ë° mixed precision ì§€ì›                       |
| **soundfile==0.13.1**       | ë©€í‹°ëª¨ë‹¬ ì˜¤ë””ì˜¤ ì…ë ¥ìš©                                             |
| **pillow==11.1.0**          | ì´ë¯¸ì§€ ì…ì¶œë ¥ìš©                                                 |
| **scipy==1.15.2**           | ì‹ í˜¸ì²˜ë¦¬ ë° ì˜¤ë””ì˜¤ ë³€í™˜ ì‹œ ì‚¬ìš©                                       |
| **torchvision==0.21.0**     | ë¹„ì „ ëª¨ë“ˆìš©                                                   |
| **backoff==2.2.1**          | ë‚´ë¶€ API ì¬ì‹œë„ìš© ìœ í‹¸ë¦¬í‹°                                         |
| **peft==0.13.2**            | LoRA ê°€ì¤‘ì¹˜ í†µí•©ìš©                                             |

# First Test - basic
(flash-attn ë¯¸ì„¤ì¹˜)
#### phi_ï¼”.py
```python
from transformers import AutoProcessor, AutoModelForCausalLM, AutoConfig
from PIL import Image
import torch
import json
import time

# ====== ì„¤ì • ======
model_id = "microsoft/Phi-4-multimodal-instruct"

device = "cuda" if torch.cuda.is_available() else "cpu" # MX450 OOM ë°œìƒ

# ====== ëª¨ë¸ ë¡œë“œ ======
print("ğŸ”¹ ëª¨ë¸ ë¡œë“œ ì¤‘...")
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16 if device == "cuda" else torch.float32,
    device_map="auto",
    attn_implementation="eager",  # FlashAttention ë¹„í™œì„±í™”
    trust_remote_code=True,       # ë³´ì•ˆ í™•ì¸ í”„ë¡¬í”„íŠ¸ y ì²˜ë¦¬
)
processor = AutoProcessor.from_pretrained(model_id)

# ====== ì…ë ¥ ======
prompt = "ì´ ì´ë¯¸ì§€ë¥¼ ë³´ê³  ê¸°ì‚¬ ë‚´ìš©ì„ ìš”ì•½í•´ì¤˜. <|image_1|>"
image = Image.open("test_data/image/test_article1.png").resize((512,512))

inputs = processor(prompt, images=image, return_tensors="pt").to(device)

# ====== ì¶”ë¡  ======
print("ğŸ§  ì¶”ë¡  ì‹œì‘...")
start = time.time()
outputs = model.generate(**inputs, max_new_tokens=200)
end = time.time()

# ====== ì¶œë ¥ ======
result = processor.batch_decode(outputs, skip_special_tokens=True)[0]
print("âœ… ê²°ê³¼:", result)
print(f"â±ï¸ ì¶”ë¡  ì‹œê°„: {end - start:.2f}ì´ˆ")

# ====== ì €ì¥ ======
_os.makedirs("outputs", exist_ok=True)
with open("outputs/result.json", "w", encoding="utf-8") as f:
    json.dump({"prompt": prompt, "result": result}, f, ensure_ascii=False, indent=2)
```

### ê²°ê³¼
![[Pasted image 20251006130539.png]]
(Error ë‚´ìš© í™•ëŒ€)
![[Pasted image 20251006130732.png]]
FlashAttention ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ì§€ ì•Šì•˜ì§€ë§Œ, ëª¨ë¸ ìì²´ì—ì„œ í•´ë‹¹ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í˜¸ì¶œí•´ì„œ ìƒìš”í•˜ê¸° ë•Œë¬¸ì— 
``` python
attn_implementation="eager"
```
ì„¤ì •ì„ í•´ë„ ë„˜ì–´ê°€ ì§€ì§€ê°€ ì•ŠëŠ”ë‹¤.

### Solution 1

flash-atten ë¥¼ ì„¤ì¹˜ë§Œ í•˜ê³ , ì‚¬ìš© ì„¤ì •ì€ ê·¸ëŒ€ë¡œ êº¼ë‘ê¸°

![[Pasted image 20251006131127.png]]
ë¬¸ì œê°€ ìƒê²¼ê¸¸ë˜ ì•Œì•„ ë³´ë‹ˆ **CUDA Toolkit**ì˜ ë¶€ì¬ë¡œ ë°œìƒí–ˆë‹¤ê³  í•œë‹¤.
FlashAttention ì€ PyPI íŒ¨í‚¤ì§€ ì´ì§€ë§Œ 
**ì‹¤ì œë¡œëŠ” 'CUDA C++' ì»¤ë„ì„ ë¡œì»¬ì—ì„œ ì»´íŒŒì¼í•´ì•¼ í•˜ëŠ” ì†ŒìŠ¤ ë°°í¬í˜• íŒ¨í‚¤ì§€'** ë¼ì„œ 
CUDA Toolkit ì„ ìš”êµ¬í•œë‹¤
(ì´ ê¸€ ì‘ì„±ì‹œì ì—ì„œ Toolkit ë²„ì „ì€ 11.7 ì„ ìš”êµ¬í•˜ê³  ìˆìŒ)


```
ì´ ì‹œì ì—ì„œ windows ì— ê·¸ë˜í”½ ë“œë¼ì´ë²„ ì„¤ì¹˜ì™€ tooklit ì„¤ì¹˜ ë° wsl ì—ì„œ ë“œë¼ì´ë²„ ì„¤ì¹˜ì™€ tooklit ì„¤ì¹˜ì— ê´€í•´ ì•Œì•„ë³´ê³  ì •ë¦¬í•´ë³´ì•˜ë‹¤

ê²°ë¡ : 
windows ì‚¬ìš©ìëŠ” window ì— ë“œë¼ì´ë²„ë¥¼ ì„¤ì¹˜í•˜ê³ ,

GPU ì‘ì—…ì´ windows OS ì—ì„œ ì´ë¤„ì§„ë‹¤ë©´ windows ì—ë‹¤ê°€ toolkit ì„ ì„¤ì¹˜í•˜ê³ ,
GPU ì‘ì—…ì´ WSL ì—ì„œ ì´ë¤„ì§„ë‹¤ë©´ WSL ì—ë‹¤ê°€ toolkit ì„ ì„¤ì¹˜í•œë‹¤

ì–´ë–¤ ìª½ì— toolkit ì„ ì‚¬ìš©í•˜ë”ë¼ë„, ê·¸ ìƒìœ„ëŠ” windows ì— ì„¤ì¹˜í•œ ë“œë¼ì´ë²„ë¥¼ í†µí•´ì„œ ì‘ì—…ì´ ì´ë¤„ì§„ë‹¤
```


### Solution 1-1. GPU ë“œë¼ì´ë²„ ë° toolkit ì¬ì„¤ì • í›„ flash_attn ì„¤ì¹˜

```
(phi-4) redjh@red-laptop:~$ pip install flash_attn==2.7.4.post1
Collecting flash_attn==2.7.4.post1
  Using cached flash_attn-2.7.4.post1.tar.gz (6.0 MB)
  Preparing metadata (setup.py) ... done
Requirement already satisfied: torch in ./miniconda3/envs/phi-4/lib/python3.10/site-packages (from flash_attn==2.7.4.post1) (2.6.0+cu124)
Collecting einops (from flash_attn==2.7.4.post1)
  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)
Requirement already satisfied: filelock in ./miniconda3/envs/phi-4/lib/python3.10/site-packages (from torch->flash_attn==2.7.4.post1) (3.19.1)
Requirement already satisfied: typing-extensions>=4.10.0 in ./miniconda3/envs/phi-4/lib/python3.10/site-packages (from torch->flash_attn==2.7.4.post1) (4.15.0)
Requirement already satisfied: networkx in ./miniconda3/envs/phi-4/lib/python3.10/site-packages (from torch->flash_attn==2.7.4.post1) (3.3)
Requirement already satisfied: jinja2 in ./miniconda3/envs/phi-4/lib/python3.10/site-packages (from torch->flash_attn==2.7.4.post1) (3.1.6)
Requirement already satisfied: fsspec in ./miniconda3/envs/phi-4/lib/python3.10/site-packages (from torch->flash_attn==2.7.4.post1) (2025.9.0)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./miniconda3/envs/phi-4/lib/python3.10/site-packages (from torch->flash_attn==2.7.4.post1) (12.4.127)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./miniconda3/envs/phi-4/lib/python3.10/site-packages (from torch->flash_attn==2.7.4.post1) (12.4.127)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./miniconda3/envs/phi-4/lib/python3.10/site-packages (from torch->flash_attn==2.7.4.post1) (12.4.127)
Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./miniconda3/envs/phi-4/lib/python3.10/site-packages (from torch->flash_attn==2.7.4.post1) (9.1.0.70)
Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./miniconda3/envs/phi-4/lib/python3.10/site-packages (from torch->flash_attn==2.7.4.post1) (12.4.5.8)
Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./miniconda3/envs/phi-4/lib/python3.10/site-packages (from torch->flash_attn==2.7.4.post1) (11.2.1.3)
Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./miniconda3/envs/phi-4/lib/python3.10/site-packages (from torch->flash_attn==2.7.4.post1) (10.3.5.147)
Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./miniconda3/envs/phi-4/lib/python3.10/site-packages (from torch->flash_attn==2.7.4.post1) (11.6.1.9)
Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./miniconda3/envs/phi-4/lib/python3.10/site-packages (from torch->flash_attn==2.7.4.post1) (12.3.1.170)
Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in ./miniconda3/envs/phi-4/lib/python3.10/site-packages (from torch->flash_attn==2.7.4.post1) (0.6.2)
Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./miniconda3/envs/phi-4/lib/python3.10/site-packages (from torch->flash_attn==2.7.4.post1) (2.21.5)
Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./miniconda3/envs/phi-4/lib/python3.10/site-packages (from torch->flash_attn==2.7.4.post1) (12.4.127)
Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./miniconda3/envs/phi-4/lib/python3.10/site-packages (from torch->flash_attn==2.7.4.post1) (12.4.127)
Requirement already satisfied: triton==3.2.0 in ./miniconda3/envs/phi-4/lib/python3.10/site-packages (from torch->flash_attn==2.7.4.post1) (3.2.0)
Requirement already satisfied: sympy==1.13.1 in ./miniconda3/envs/phi-4/lib/python3.10/site-packages (from torch->flash_attn==2.7.4.post1) (1.13.1)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./miniconda3/envs/phi-4/lib/python3.10/site-packages (from sympy==1.13.1->torch->flash_attn==2.7.4.post1) (1.3.0)
Requirement already satisfied: MarkupSafe>=2.0 in ./miniconda3/envs/phi-4/lib/python3.10/site-packages (from jinja2->torch->flash_attn==2.7.4.post1) (2.1.5)
Downloading einops-0.8.1-py3-none-any.whl (64 kB)
Building wheels for collected packages: flash_attn
  DEPRECATION: Building 'flash_attn' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'flash_attn'. Discussion can be found at https://github.com/pypa/pip/issues/6334
  Building wheel for flash_attn (setup.py) ... done
  Created wheel for flash_attn: filename=flash_attn-2.7.4.post1-cp310-cp310-linux_x86_64.whl size=187815087 sha256=ffe17686fa1a0f288de9eae7c32af209d32a27b037ef28614f042b377af5b15a
  Stored in directory: /home/redjh/.cache/pip/wheels/59/ce/d5/08ea07bfc16ba218dc65a3a7ef9b6a270530bcbd2cea2ee1ca
Successfully built flash_attn
Installing collected packages: einops, flash_attn
Successfully installed einops-0.8.1 flash_attn-2.7.4.post1
```

flash-attn ì„¤ì¹˜ í›„ ì¬ì‹¤í–‰

```
ğŸ”¹ ëª¨ë¸ ë¡œë“œ ì¤‘...
config.json: 4.63kB [00:00, 11.4MB/s]
configuration_phi4mm.py: 11.0kB [00:00, 37.0MB/s]
A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-4-multimodal-instruct:
- configuration_phi4mm.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
modeling_phi4mm.py: 116kB [00:00, 50.2MB/s]
vision_siglip_navit.py: 78.2kB [00:00, 103MB/s]
A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-4-multimodal-instruct:
- vision_siglip_navit.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
processing_phi4mm.py: 32.8kB [00:00, 59.3MB/s]
A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-4-multimodal-instruct:
- processing_phi4mm.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
speech_conformer_encoder.py: 111kB [00:00, 54.6MB/s]
A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-4-multimodal-instruct:
- speech_conformer_encoder.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-4-multimodal-instruct:
- modeling_phi4mm.py
- vision_siglip_navit.py
- processing_phi4mm.py
- speech_conformer_encoder.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
/home/redjh/miniconda3/envs/phi-4/lib/python3.10/site-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
model.safetensors.index.json: 240kB [00:00, 142MB/s]
model-00001-of-00003.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.00G/5.00G [04:37<00:00, 18.0MB/s]
model-00002-of-00003.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.95G/4.95G [04:29<00:00, 18.4MB/s]
model-00003-of-00003.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.20G/1.20G [01:07<00:00, 17.8MB/s]
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [10:15<00:00, 205.33s/it]
/home/redjh/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/33e62acdd07cd7d6635badd529aa0a3467bb9c6a/speech_conformer_encoder.py:2774: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.
  lambda i: encoder_checkpoint_wrapper(
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:18<00:00,  6.08s/it]
generation_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 190/190 [00:00<00:00, 1.42MB/s]
Some parameters are on the meta device because they were offloaded to the disk and cpu.
processor_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 121/121 [00:00<00:00, 1.23MB/s]
preprocessor_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 482/482 [00:00<00:00, 4.35MB/s]
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
tokenizer_config.json: 3.25kB [00:00, 2.53MB/s]
vocab.json: 3.91MB [00:00, 31.6MB/s]
merges.txt: 2.42MB [00:00, 33.0MB/s]
tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15.5M/15.5M [00:02<00:00, 7.58MB/s]
added_tokens.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 249/249 [00:00<00:00, 1.12MB/s]
special_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 473/473 [00:00<00:00, 3.63MB/s]
```
```
ğŸ§  ì¶”ë¡  ì‹œì‘...
/home/redjh/miniconda3/envs/phi-4/lib/python3.10/site-packages/transformers/generation/utils.py:2137: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.
  warnings.warn(
Traceback (most recent call last):
  File "/home/redjh/phi_4_multi/phi_4.py", line 32, in <module>
    outputs = model.generate(**inputs, max_new_tokens=150)
  File "/home/redjh/miniconda3/envs/phi-4/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/redjh/miniconda3/envs/phi-4/lib/python3.10/site-packages/transformers/generation/utils.py", line 2255, in generate
    result = self._sample(
  File "/home/redjh/miniconda3/envs/phi-4/lib/python3.10/site-packages/transformers/generation/utils.py", line 3254, in _sample
    outputs = self(**model_inputs, return_dict=True)
  File "/home/redjh/miniconda3/envs/phi-4/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/redjh/miniconda3/envs/phi-4/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/redjh/miniconda3/envs/phi-4/lib/python3.10/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/redjh/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/33e62acdd07cd7d6635badd529aa0a3467bb9c6a/modeling_phi4mm.py", line 2116, in forward
    outputs = self.model(
  File "/home/redjh/miniconda3/envs/phi-4/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/redjh/miniconda3/envs/phi-4/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/redjh/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/33e62acdd07cd7d6635badd529aa0a3467bb9c6a/modeling_phi4mm.py", line 1707, in forward
    inputs_embeds = self.embed_tokens_extend(
  File "/home/redjh/miniconda3/envs/phi-4/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/redjh/miniconda3/envs/phi-4/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/redjh/miniconda3/envs/phi-4/lib/python3.10/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/redjh/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/33e62acdd07cd7d6635badd529aa0a3467bb9c6a/modeling_phi4mm.py", line 769, in forward
    image_hidden_states = self.image_embed(
  File "/home/redjh/miniconda3/envs/phi-4/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/redjh/miniconda3/envs/phi-4/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/redjh/miniconda3/envs/phi-4/lib/python3.10/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/redjh/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/33e62acdd07cd7d6635badd529aa0a3467bb9c6a/modeling_phi4mm.py", line 328, in forward
    img_features = self.get_img_features(img_embeds.flatten(0, 1), attention_mask=image_attention_mask.type(torch.BoolTensor).flatten(0,1).to(target_device))
  File "/home/redjh/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/33e62acdd07cd7d6635badd529aa0a3467bb9c6a/modeling_phi4mm.py", line 194, in get_img_features
    img_processor_output = self.img_processor(img_embeds, output_hidden_states=True, patch_attention_mask=attention_mask)
  File "/home/redjh/miniconda3/envs/phi-4/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/redjh/miniconda3/envs/phi-4/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/redjh/miniconda3/envs/phi-4/lib/python3.10/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/redjh/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/33e62acdd07cd7d6635badd529aa0a3467bb9c6a/vision_siglip_navit.py", line 1385, in forward
    encoder_outputs = self.encoder(
  File "/home/redjh/miniconda3/envs/phi-4/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/redjh/miniconda3/envs/phi-4/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/redjh/miniconda3/envs/phi-4/lib/python3.10/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/redjh/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/33e62acdd07cd7d6635badd529aa0a3467bb9c6a/vision_siglip_navit.py", line 1179, in forward
    layer_outputs = encoder_layer(
  File "/home/redjh/miniconda3/envs/phi-4/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/redjh/miniconda3/envs/phi-4/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/redjh/miniconda3/envs/phi-4/lib/python3.10/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/redjh/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/33e62acdd07cd7d6635badd529aa0a3467bb9c6a/vision_siglip_navit.py", line 953, in forward
    hidden_states, attn_weights = self.self_attn(
  File "/home/redjh/miniconda3/envs/phi-4/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/redjh/miniconda3/envs/phi-4/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/redjh/miniconda3/envs/phi-4/lib/python3.10/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/redjh/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/33e62acdd07cd7d6635badd529aa0a3467bb9c6a/vision_siglip_navit.py", line 797, in forward
    attn_output = self._flash_attention_forward(
  File "/home/redjh/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/33e62acdd07cd7d6635badd529aa0a3467bb9c6a/vision_siglip_navit.py", line 844, in _flash_attention_forward
    attn_output_unpad = flash_attn_varlen_func(
  File "/home/redjh/miniconda3/envs/phi-4/lib/python3.10/site-packages/flash_attn/flash_attn_interface.py", line 1448, in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
  File "/home/redjh/miniconda3/envs/phi-4/lib/python3.10/site-packages/torch/autograd/function.py", line 575, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/redjh/miniconda3/envs/phi-4/lib/python3.10/site-packages/flash_attn/flash_attn_interface.py", line 930, in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
  File "/home/redjh/miniconda3/envs/phi-4/lib/python3.10/site-packages/torch/_ops.py", line 1123, in __call__
    return self._op(*args, **(kwargs or {}))
  File "/home/redjh/miniconda3/envs/phi-4/lib/python3.10/site-packages/torch/_library/autograd.py", line 113, in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
  File "/home/redjh/miniconda3/envs/phi-4/lib/python3.10/site-packages/torch/_library/autograd.py", line 40, in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
  File "/home/redjh/miniconda3/envs/phi-4/lib/python3.10/site-packages/torch/_ops.py", line 728, in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)
NotImplementedError: Could not run 'flash_attn::_flash_attn_varlen_forward' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'flash_attn::_flash_attn_varlen_forward' is only available for these backends: [CUDA, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

CUDA: registered at /dev/null:173 [kernel]
Meta: registered at /dev/null:198 [kernel]
BackendSelect: fallthrough registered at /pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:194 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at /pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:503 [backend fallback]
Functionalize: registered at /pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:349 [backend fallback]
Named: registered at /pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at /pytorch/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at /pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at /pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:100 [backend fallback]
AutogradOther: registered at /dev/null:173 [autograd kernel]
AutogradCPU: registered at /dev/null:173 [autograd kernel]
AutogradCUDA: registered at /dev/null:173 [autograd kernel]
AutogradHIP: registered at /dev/null:173 [autograd kernel]
AutogradXLA: registered at /dev/null:173 [autograd kernel]
AutogradMPS: registered at /dev/null:173 [autograd kernel]
AutogradIPU: registered at /dev/null:173 [autograd kernel]
AutogradXPU: registered at /dev/null:173 [autograd kernel]
AutogradHPU: registered at /dev/null:173 [autograd kernel]
AutogradVE: registered at /dev/null:173 [autograd kernel]
AutogradLazy: registered at /dev/null:173 [autograd kernel]
AutogradMTIA: registered at /dev/null:173 [autograd kernel]
AutogradPrivateUse1: registered at /dev/null:173 [autograd kernel]
AutogradPrivateUse2: registered at /dev/null:173 [autograd kernel]
AutogradPrivateUse3: registered at /dev/null:173 [autograd kernel]
AutogradMeta: registered at /dev/null:173 [autograd kernel]
AutogradNestedTensor: registered at /dev/null:173 [autograd kernel]
Tracer: registered at /pytorch/torch/csrc/autograd/TraceTypeManual.cpp:294 [backend fallback]
AutocastCPU: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:322 [backend fallback]
AutocastXPU: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:465 [backend fallback]
AutocastMPS: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:209 [backend fallback]
AutocastCUDA: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:165 [backend fallback]
FuncTorchBatched: registered at /pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:731 [backend fallback]
BatchedNestedTensor: registered at /pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]
FuncTorchVmapMode: fallthrough registered at /pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]
Batched: registered at /pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at /pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at /pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:207 [backend fallback]
PythonTLSSnapshot: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:202 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at /pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:499 [backend fallback]
PreDispatch: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:206 [backend fallback]
PythonDispatcher: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:198 [backend fallback]
```

ì¶”ë¡ ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒ

ì½”ë“œ ë‚´ë¶€ì—ì„œ attn_flash í˜¸ì¶œì„ ì œì•½ ì‹œì¼°ìŒì—ë„ ë¶ˆêµ¬í•˜ê³  attn_flash í˜¸ì¶œë¡œ ì¸í•œ ì—ëŸ¬ê°€ ë°œìƒ

\- `attn_implementation="eager"`ì˜ ì˜ë¯¸

`transformers`ì˜ ëª¨ë“  ëª¨ë¸ì€ attention ê³„ì‚° ë°©ì‹ì„ ë‹¤ìŒ 3ê°€ì§€ ì¤‘ í•˜ë‚˜ë¡œ ì„¤ì •í•  ìˆ˜ ìˆë‹¤:

| ì˜µì…˜                    | ì˜ë¯¸                              | FlashAttention ì‚¬ìš© ì—¬ë¶€ |
| --------------------- | ------------------------------- | -------------------- |
| `"flash_attention_2"` | FlashAttention v2 ì‚¬ìš©            | ì‚¬ìš©                   |
| `"flash_attention"`   | FlashAttention v1 ì‚¬ìš©            | ì‚¬ìš©                   |
| `"eager"`             | PyTorchì˜ ê¸°ë³¸ attention kernel ì‚¬ìš© | ì‚¬ìš© ì•ˆ í•¨               |

flash_attention ì€ **'SM80 ì•„í‚¤í…ì²˜ ì½”ì–´(Ampere)'** ë¶€í„° ì§€ì›ì´ ê°€ëŠ¥í•œ kernel ì´ê¸° ë•Œë¬¸ì—, 
í˜„ì¬ local ì˜ MX450(SM75, Turing) ì—ì„œëŠ” ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” êµ¬ì¡°ë¼ ê¸°ë³¸ kernel ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ 'eager' ëª¨ë“œë¡œ ì„¤ì •í•´ì•¼ í•œë‹¤.


ChatGPT ë¥¼ í†µí•´ í•´ë‹¹ ì˜¤ë¥˜ë¥¼ í™•ì¸í•´ë³´ì•˜ë‹¤.
```
File ".../vision_siglip_navit.py", line 844, in _flash_attention_forward
    attn_output_unpad = flash_attn_varlen_func(
...
NotImplementedError: Could not run 'flash_attn::_flash_attn_varlen_forward' with arguments from the 'CPU' backend.
```
ì—¬ê¸°ì„œ í˜¸ì¶œëœ ê±´ **`vision_siglip_navit.py` ë‚´ë¶€ì˜ `_flash_attention_forward()`** ë¼ê³  í•œë‹¤

- **Phi-4ì˜ í…ìŠ¤íŠ¸ ëª¨ë¸ ë¶€ë¶„ì´ ì•„ë‹ˆë¼, ë¹„ì „ ì¸ì½”ë”(`SigLIP-NaViT`) ìª½**ì—ì„œ FlashAttentionì„ ì‹œë„

```
Phi-4-multimodal-instruct

Phi4MultiModalModel
â”œâ”€â”€ text_model (Phi4ForCausalLM)
â”‚     â””â”€â”€ attn_implementation="eager"    <- ì ìš©ë¨
â””â”€â”€ image_model (SigLIP-NaViT backbone)
      â””â”€â”€ vision encoder layers (use flash_attn by default not overridden) <- ë¬¸ì œ
```

ìš”ì•½í•˜ìë©´

- í…ìŠ¤íŠ¸ìš© Transformer ì—ëŠ” "eager" ê°€ ì ìš©ë˜ì–´ attn_flash ë¹„í™œì„±í™” ë¨
	
	- ì¼ë°˜ PyTorchì˜ `scaled_dot_product_attention`ìœ¼ë¡œ ë™ì‘.
	
- **ë¹„ì „ ì¸ì½”ë”**(ì´ë¯¸ì§€ ì¸ë² ë”©) **SigLIP-NaViT** ìª½ì€ ë³„ë„ì˜ ì„¤ì •ì„ ê°€ì§€ê³  ìˆì–´ ë”°ë¡œ ì„¤ì •ì´ í•„ìš”
	
	- `config.vision_config` í†µí•´ ì„¤ì •
	- `attn_implementation` í•„ë“œê°€ ì•„ì˜ˆ ì—†ê±°ë‚˜ 
	  `"flash_attention_2"`ë¡œ ê¸°ë³¸ê°’ ì„¤ì •ì´ ëœ ê²ƒìœ¼ë¡œ ìœ ì¶”í•  ìˆ˜ ìˆë‹¤ê³  í•¨
	- ìë£Œí˜•ì— ë”°ë¼ ê°ê°ì˜ flash_attn ì„¤ì •ì´ ì¡´ì¬í•˜ëŠ” ê²ƒìœ¼ë¡œ ìœ ì¶”
		- í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤, ë¹„ë””ì˜¤
	
- ê·¸ë˜ì„œ ì´ë¯¸ì§€ ì²˜ë¦¬ ìª½ì—ëŠ” "eager" ì²˜ë¦¬ê°€ ì ìš©ì´ ë˜ì§€ ì•ŠìŒ
	
	- ê·¸ë¡œì¸í•´ `SigLIP-NaViT`ëŠ” ìì²´ì ìœ¼ë¡œ `flash_attn` ëª¨ë“ˆì„ importí•˜ê³ ,
	  ë‚´ë¶€ í•¨ìˆ˜ `_flash_attention_forward()`ì—ì„œ FlashAttention CUDA ì»¤ë„ì„ í˜¸ì¶œí•¨


í˜„ì¬ ëª¨ë¸ì´ CPUì— ìˆê³ ,  FlashAttentionì€ GPU ì „ìš© ì»¤ë„ì´ë¯€ë¡œ, CPUì—ì„œ ì´ í•¨ìˆ˜ê°€ í˜¸ì¶œë˜ë‹ˆ
 
```
NotImplementedError: Could not run 'flash_attn::_flash_attn_varlen_forward' with arguments from the 'CPU' backend.
```
ì´ì™€ ê°™ì€ ì˜¤ë¥˜ê°€ ë™ë°˜í•œ ìƒí™©ì´ë¼ê³  ë³´ê³  ìˆë‹¤.



### Solution 1-2. í™˜ê²½ ë³€ìˆ˜ì™€ config ì œì–´ë¥¼ í†µí•´ í•´ê²°

ChatGPT ì™€ í† ë¡ ì„ í†µí•´ ë„ì¶œí•œ ì†”ë£¨ì…˜ì„ ì ìš©ì‹œì¼œ ë³´ê³ ì í•œë‹¤
### í•´ê²° ë°©ë²• ë° ì›ë¦¬

1. **FlashAttention ëª¨ë“ˆ import ìì²´ë¥¼ ì°¨ë‹¨**  
    - `sys.modules["flash_attn"] = None`, í™˜ê²½ë³€ìˆ˜ ë¹„í™œì„±í™” 
	    - (ëª¨ë¸ ë¡œë“œì‹œ `flash_attn` import ë¶ˆê°€)
    
2. **Vision ì¸ì½”ë” configì—ë„ `"eager"` ì§ì ‘ ì£¼ì…**  
    - `cfg.vision_config.attn_implementation = "eager"`
	    - (FlashAttention ê²½ë¡œê°€ `_flash_attention_forward()`ì—ì„œ `eager`ë¡œ ëŒ€ì²´ë¨)


ì‹¤ì œë¡œ try í•´ë³´ë‹ˆ **1ë²ˆ ì†”ë£¨ì…˜ ë‹¨ì¼**ë¡œëŠ” í•´ê²°ì´ ë˜ì§€ ì•Šì•˜ì§€ë§Œ.
**2ë²ˆ ì†”ë£¨ì…˜ì„ ë‹¨ì¼**ë¡œ ì‚¬ìš©í•˜ë‹ˆ flash_attn í˜¸ì¶œ ì¸í•œ ì—ëŸ¬ê°€ ì—†ì–´ì§„ ê²ƒì„ í™•ì¸ë  ìˆ˜ ìˆì—ˆë‹¤.



---
	