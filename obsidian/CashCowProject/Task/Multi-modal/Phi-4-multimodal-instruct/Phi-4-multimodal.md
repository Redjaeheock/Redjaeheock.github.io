[huggingface](https://huggingface.co/microsoft/Phi-4-multimodal-instruct)
## 모델 요약

Phi-4-multimodal-instruct는 Phi-3.5 및 4.0 모델에 사용된 언어, 비전, 음성 연구 및 데이터 세트를 활용하는 **경량 개방형 멀티모달 기반 모델**입니다. 
이 모델은 텍스트, 이미지, 오디오 입력을 처리하여 텍스트 출력을 생성하며, **128K 토큰 컨텍스트** 길이를 제공합니다. 
이 모델은 지도 학습 미세 조정, 직접 선호도 최적화, 그리고 RLHF(인간 피드백을 통한 **강화 학습**)를 모두 통합하는 개선 과정을 거쳤습니다. 각 모달이 지원하는 언어는 다음과 같습니다.

- 텍스트: 아랍어, 중국어, 체코어, 덴마크어, 네덜란드어, 영어, 핀란드어, 프랑스어, 독일어, 히브리어, 헝가리어, 이탈리아어, 일본어, 한국어, 노르웨이어, 폴란드어, 포르투갈어, 러시아어, 스페인어, 스웨덴어, 태국어, 터키어, 우크라이나어
- 비전: 영어
- 오디오: 영어, 중국어, 독일어, 프랑스어, 이탈리아어, 일본어, 스페인어, 포르투갈어

---
[Microsoft](https://azure.microsoft.com/en-us/blog/empowering-innovation-the-next-generation-of-the-phi-family)
## Phi-4-멀티모달이란 무엇인가요?

Phi-4-multimodal 은 Microsoft 최초의 멀티모달 언어 모델로서 AI 개발에 새로운 이정표를 세웠습니다. 혁신의 핵심은 지속적인 개선이며, 이는 고객의 의견에 귀 기울이는 것에서 시작됩니다. 
고객 피드백에 직접 대응하여 **음성 , 시각, 텍스트 처리를 하나의 통합 아키텍처로 완벽하게 통합**하는 **56억 개의** 매개변수 모델 인 **Phi-4-multimodal**을 개발했습니다.

이 모델은 고급 교차 모달 학습 기술을 활용하여 더욱 자연스럽고 상황 인식적인 상호작용을 가능하게 하여 기기가 여러 입력 방식을 동시에 이해하고 추론할 수 있도록 합니다. 
음성 언어 해석, 이미지 분석, 텍스트 정보 처리 등 어떤 작업을 수행하든 매우 효율적이고 지연 시간이 짧은 추론을 제공하는 동시에, 기기 내 실행을 최적화하고 **연산 오버헤드를 줄입니**다.

Phi-4-multimodal 은 음성, 시각, 언어를 포함하는 **LoRA 혼합 모델을 단일 모델**로, **동일한 표현 공간 내에서 동시에 처리**됩니다. [[LoRA#Low Rank Adaptation of Large Language Models]]
결과적으로 텍스트, 오디오, 시각 입력을 모두 처리할 수 있는 단일 통합 모델이 생성되므로 
**복잡한 파이프라인이나 다양한 모달리티에 대한 별도의 모델이 필요하지 않습니다.**

Phi-4-multimodal 은 효율성과 확장성을 향상시키는 새로운 아키텍처를 기반으로 합니다. 향상된 처리 성능을 위해 더 많은 어휘를 통합하고, 다국어 기능을 지원하며, 언어 추론을 멀티모달 입력과 통합합니다. 이 모든 기능은 **기기 및 엣지 컴퓨팅 플랫폼에 구축하기에 적합**한 강력하고 컴팩트하며 고효율적인 모델 내에서 구현됩니다.

이 모델은 Phi 제품군의 한 단계 더 발전된 모델로, 작은 크기에 향상된 성능을 제공합니다. 모바일 기기든 엣지 시스템이든 고급 AI 기능을 원하는 경우, Phi-4-multimodal 은 효율적이고 다재다능한 고성능 옵션을 제공합니다.

Phi-4-multimodal 은 

- 시각 및 청각을 모두 처리할 수 있습니다.
	- ![[Pasted image 20250923193410.png]]
	- 다음 표는 차트/표 이해 및 문서 추론 작업에서 시각 콘텐츠에 대한 입력 쿼리가 합성 음성일 때의 모델 품질을 보여줍니다. 청각 및 시각 신호를 입력으로 사용할 수 있는 기존의 다른 최첨단 옴니 모델과 비교했을 때, Phi-4-multimodal은 여러 벤치마크에서 훨씬 더 뛰어난 성능을 달성합니다.
	
- 자동 음성 인식(ASR)과 음성 번역(ST) 모두에서 *WhisperV3 및 SeamlessM4T-v2-Large* 와 같은 특수 모델을 능가합니다
	- 이 모델은 6.14%의 인상적인 단어 오류율로 [Huggingface OpenASR](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard) 리더보드에서 1위를 차지했으며, 2025년 2월 기준 이전 최고 성능인 6.5%를 넘어섰습니다
	- 또한 음성 요약을 성공적으로 구현하고 GPT-4o 모델과 비슷한 성능 수준을 달성한 몇 안 되는 개방형 모델 중 하나입니다
	
- 음성 질의응답(QA) 작업에서 *Gemini-2.0-Flash 및 GPT-4o-realtime-preview* 와 같은 근접 모델과 격차가 있는데, 모델 크기가 작을수록 사실 기반 QA 지식을 유지하는 용량이 줄어들기 때문입니다. 
	- 다음 버전에서 이 기능을 개선하기 위한 작업이 진행 중입니다.
- 56억 개의 매개변수만 사용하는 Phi-4-multimodal 은 다양한 벤치마크에서 탁월한 시각 성능을 보여주며, 특히 수학적 및 과학적 추론에서 뛰어난 성능을 달성했습니다.
	- 크기는 작지만, 문서 및 차트 이해, **광학 문자 인식(OCR), 시각적 과학 추론** 등 일반적인 멀티모달 기능에서 경쟁력 있는 성능을 유지하며, *Gemini-2-Flash-lite-preview/Claude-3.5-Sonnet* 과 같은 모델과 유사하거나 그 이상을 달성했습니다.

# ---------------------- 강약점 요약 ----------------------

## < 강점 >

- ##### 음성 처리
    - ASR(자동 음성 인식) 성능이 매우 뛰어나 단어 오류율(WER) 최저 수준 달성
    - ST(음성 번역)에서도 전문 모델 이상 성능
    - 음성 요약에서도 GPT-4o급에 근접 → 영상 요약·회의 요약 등에 강함
    
- ##### 시각 처리
    - **문서·차트·표 이해**에서 우수한 성능
    - **OCR(광학 문자 인식)** 성능 탁월
    - **시각적 과학 추론·수학적 추론**에서 강력한 결과
    
- ##### 시각 + 음성 처리
	- **합성 음성 입력**으로 주어진 차트/표/문서 질의에서도 강력한 성능을 유지
	
- ##### 멀티모달 통합
    - 작은 모델 크기(56억 파라미터)에도 불구하고 일반적인 멀티모달 작업에서 **대형 모델에 근접하거나 동등한 성능**

## < 약점 >

- ##### 사실 기반 질의응답(QA)
    - 음성 기반 질문(QA) 처리에서 상대적으로 약함
    - 이유: 작은 모델이라 **내부 지식 저장/회상 용량**이 부족 → 외부 지식 기반 질문에 취약
    
- ##### 모델 크기 한계
    - 희귀 지식(long-tail knowledge)이나 **복잡한 사실 관계 추론**에서 한계 노출
    - 대형 모델에 비해 **사실적 정확성**(factual accuracy)에서 격차 발생

## 사용자 정의 및 크로스 플랫폼

Phi-4-mini와 Phi-4-multimodal 모델은 크기가 작기 때문에 컴퓨팅 제약이 있는 추론 환경에서 사용할 수 있습니다. 
이러한 모델은 특히 **ONNX 런타임을 통해 크로스 플랫폼 가용성을 위해 추가 최적화될 경우 온디맨드(on-device) 방식으로 사용할 수 있습니다**. 
연산 요구량이 낮아 지연 시간이 훨씬 짧고 비용도 절감됩니다. 
더 긴 컨텍스트 윈도우 덕분에 문서, 웹 페이지, 코드 등 대용량 텍스트 콘텐츠를 처리하고 추론할 수 있습니다. 
Phi-4-mini 와 multimodal 모델은 강력한 추론 및 논리 성능을 보여주어 분석 작업에 적합합니다. 또한, 크기가 작기 때문에 미세 조정이나 사용자 지정이 더 쉽고 저렴합니다. 아래 표는 Phi-4-multimodal 을 사용한 미세 조정 시나리오의 예를 보여줍니다.

![[Pasted image 20250923195137.png]]

이러한 모델은 복잡한 작업을 효율적으로 처리하도록 설계되어, 예외적인 상황과 컴퓨팅이 제한된 환경에 이상적입니다. Phi-4-multimodal과 Phi-4-mini의 새로운 기능을 고려하면 Phi의 활용 범위는 더욱 확대되고 있습니다. Phi 모델은 AI 생태계에 통합되어 다양한 산업 분야의 활용 사례를 탐색하는 데 활용되고 있습니다

_언어 모델은 강력한 추론 엔진이며, Phi와 같은 소규모 언어 모델을 Windows에 통합하면 효율적인 컴퓨팅 기능을 유지하고 모든 앱과 경험에 걸쳐 지속적인 인텔리전스를 구현하는 미래로 나아갈 수 있습니다. Copilot+ PC는 Phi-4-멀티모달의 기능을 기반으로 구축되어 에너지 소모 없이 Microsoft의 고급 SLM 기능을 제공합니다. 이러한 통합은 생산성, 창의성, 그리고 교육 중심 경험을 향상시켜 개발자 플랫폼의 표준으로 자리매김할 것입니다._

— Vivek Pradeep, Windows 응용 과학 부문 부사장 겸 수석 엔지니어.

---
[Arxiv](https://arxiv.org/abs/2503.01743)
## Phi-4-Mini 기술 보고서: 
## LoRA 혼합을 통한 간결하면서도 강력한 다중 모드 언어 모델

작지만 뛰어난 성능을 자랑하는 언어 및 멀티모달 모델인 Phi-4-Mini와 Phi-4-Multimodal을 소개합니다. 

Phi-4-Mini는 ... 제공합니다. 

Phi-4-Multimodal은 텍스트, 비전, 음성/오디오 입력 방식을 단일 모델로 통합하는 멀티모달 모델입니다. 

새로운 모달리티 확장 방식은 LoRA 어댑터와 모달리티별 라우터를 활용하여 간섭 없이 다양한 모달리티를 결합하는 다중 추론 모드를 지원합니다. 
예를 들어, 음성/오디오 모달리티의 LoRA 구성 요소는 4억 6천만 개의 매개변수를 가지고 있음에도 불구하고, 현재 OpenASR 리더보드에서 1위를 차지하고 있습니다. 

Phi-4-Multimodal은 (시각 + 언어), (시각 + 음성), 그리고 (음성/오디오) 입력과 관련된 시나리오를 지원하여 광범위한 작업에서 대규모 시각-언어 및 음성-언어 모델을 능가합니다. 
또한, Phi-4-Mini의 추론 기능을 향상시키기 위해 추가 학습을 위한 실험을 진행하고 있습니다. 

38억 개의 매개변수를 가진 작은 크기에도 불구하고, 이 실험 버전은 DeepSeek-R1-Distill-Qwen-7B 및 DeepSeek-R1-Distill-Llama-8B를 포함한 훨씬 더 큰 모델과 동등하거나 훨씬 더 뛰어난 추론 성능을 달성합니다

---




